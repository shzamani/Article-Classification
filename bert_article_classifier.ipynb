{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Isentia Challenge -  LDA + BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sPWldBgB9N5-",
        "cZcEdUQ8qkJh",
        "aOceZNYN8Aa3",
        "TGrzNbVb9uzX",
        "psSe3gg0FYP3",
        "hP59zox_Pa3m",
        "0sg_3CD5Qvjn",
        "6Ma0Xg-etyix",
        "us8jrMIJeY9T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAYcniO3Ptyx",
        "colab_type": "text"
      },
      "source": [
        "### Problem Description:\n",
        "As a media monitoring company, we try to make sense of the media that is created every day. One area in particular is written articles, where we want to automatically determine the type of the article. This knowledge helps us to filter articles and present to our customers only what they really care about.\n",
        "You are given a corpus of text articles, included in this repo.\n",
        "\n",
        "Your job is to create a service that **classifies** the *articles* into related groups by detecting patterns inside the dataset. When your service is given a new article, your model(s) should return the *type(s)* that this article belongs to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hS9r7iVQLOH",
        "colab_type": "text"
      },
      "source": [
        "### Plan of attack:\n",
        "* Split the lines from text file to get articles and their labels\n",
        "* Preprocess articles: remove stop words, punctuations, etc\n",
        "* As it is mentioned in the problem description, associated article labels are not reliable, thus we need to use technique to define true article labels. For this challenge I use LDA since it is a well-known model for topic modeling. I have used LDA to cluster articles into topics based on their bag of words.\n",
        "* Add true labels/topics to the dataset\n",
        "* Build a transformer based classifier to detect long range dependencies/patterns within articles, the issue that I faced while trying to use RNN models such as LSTM and bi-LSTM for this challenge. I decided to use BERT model to tackle this issue.\n",
        "* You can also try the model as a service in [here](http://drstrange.cse.unsw.edu.au:5002).\n",
        "\n",
        "* Results and observations\n",
        "  * The transformer model achieved 83% accuracy in classifying articles by getting trained on only 50k out of 260k available samples. I use this amount of training data just to speed up the training step. However, if we want to use this model in production, it is required to train it on the whole dataset. Furthermore, we can also generate more training data by leveraging text augmentation techniques (e.g. pivot language paraphrasing, synonym replacement).\n",
        "  * The main issue with transformer models is that they are very slow (due to huge number of parameters) at inference time. One solution to speed up the inference is to use [distillation](https://medium.com/pytorch/bert-distillation-with-catalyst-c6f30c985854) technique. The main idea is to use a model (with less number of parameters) that can achieve almost same performance (with slight decrease) compare to the larger model. Another solution is to use pooled representation of sentences ([CLS] special character) from third or second to the last layer in BERT architecture, and pass these representations to a simpler model (e.g. Random Forest). You can find an example of such solution in [here](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/). This solution works well if the problem is classification (similar to what we have in this challenge)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPWldBgB9N5-",
        "colab_type": "text"
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp_Tt-iPrDGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3b7dee1-1bb7-4f95-e645-96a6def1972f"
      },
      "source": [
        "!pip install gensim\n",
        "!pip install wordcloud\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!pip install tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.48)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.48)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from wordcloud) (1.18.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZcEdUQ8qkJh",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgHLfujnPpHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim import corpora, models\n",
        "from gensim.test.utils import datapath\n",
        "# import nltk\n",
        "# from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "# nltk.download('wordnet')\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
        "from transformers import BertModel, BertTokenizer, BertConfig, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
        "import time\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
        "PATH = \".\""
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOceZNYN8Aa3",
        "colab_type": "text"
      },
      "source": [
        "### Prepare articles to train our transformer model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skSf0GtdBimm",
        "colab_type": "text"
      },
      "source": [
        "We have the option: \n",
        "\n",
        "\n",
        "1.   Preprocess raw articles and assign appropriate topic labels to each\n",
        "2.   Download *ready-to-use* articles from [here](https://www.dropbox.com/s/qbkm39innbud1c4/articles.csv?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGrzNbVb9uzX",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Preprocess raw articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf1eujcxCGJy",
        "colab_type": "text"
      },
      "source": [
        "Read [text file](https://bitbucket.org/isentia/coding-challenge-ml/src/master/) contains articles and their labels. We only use train.txt file for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGOhmwgE7_RO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"train.txt\"\n",
        "lines = []\n",
        "with open(os.path.join(PATH, filename), \"r\") as file:\n",
        "  for line in file.readlines():\n",
        "    lines.append(line)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPGYYDoe8FB_",
        "colab_type": "text"
      },
      "source": [
        "Extract article texts and corresponding labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so2K9mTk8EXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles = []\n",
        "labels = []\n",
        "num_topics = 0 # number of unique labels assigned to articles (can be found in the text file)\n",
        "for line in lines:\n",
        "  articles.append(line.split(\"__label__\")[0])\n",
        "  labels.append(line.split(\"__label__\")[1].replace(\"\\n\", \"\"))\n",
        "num_topics = len(set(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpeGT7Kz8XI7",
        "colab_type": "text"
      },
      "source": [
        "Preprocess articles: \n",
        "*   remove stop words\n",
        "*   filter out words with less than 3 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQHxtPoD8W0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we skip the stemming step for now\n",
        "# stemmer = SnowballStemmer(\"english\")\n",
        "# def lemmatize_stemming(article):\n",
        "#     return stemmer.stem(WordNetLemmatizer().lemmatize(article, pos='v'))\n",
        "\n",
        "def preprocess(article):\n",
        "    result = []\n",
        "    article = re.sub('[,\\.!?]', '', article)\n",
        "    for word in gensim.utils.simple_preprocess(article):\n",
        "        if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 3: # remove stop words and words with less than 3 characters\n",
        "            result.append(word)\n",
        "    return result\n",
        "\n",
        "# preprocess articles\n",
        "processed_articles = []\n",
        "for article in articles:\n",
        "  processed_articles.append(preprocess(article))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB0zyKSYOyrc",
        "colab_type": "text"
      },
      "source": [
        "Create a dictionary of most frequent words, we will use it later when categorise articles into topics (topic modelling). Inspired by [this](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24), we filter out words that appear in:\n",
        "* less than 15 articles (absolute number) or\n",
        "* more than 0.5 articles (fraction of total corpus size, not absolute number).\n",
        "* after the above two steps, keep only the first 100k most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-9TU41bO4Av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_articles)\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiQ3R4hBO5sr",
        "colab_type": "text"
      },
      "source": [
        "Create BoW to replace each word appears in an article with its index from the dictionary (convert corpus to BoW format)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4heCrYmO7vA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_articles = [dictionary.doc2bow(bow_article) for bow_article in processed_articles]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_E6aP7jO9mj",
        "colab_type": "text"
      },
      "source": [
        "Compute TF-IDF for each BoW formatted article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCzMOir1O_S2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_model = models.TfidfModel(bow_articles)\n",
        "tfidf_converted_articles = tfidf_model[bow_articles]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OBOjahqKECl",
        "colab_type": "text"
      },
      "source": [
        "##### Use LDA to form topics and categorise articles based on their bag-of-words. As it is mentioned in the problem description, initial labels assigned to articles are not reliable, thus, we use LDA model to classify each article into an appropriate topic and consider that topic as its true label.\n",
        "\n",
        "We have two options: \n",
        "-   Create LDA model\n",
        "-   Download *ready-to-use* model from [here](https://www.dropbox.com/s/t3td12qefngddo0/isentia-lda-model.zip?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLOCQfgiH2_Y",
        "colab_type": "text"
      },
      "source": [
        "###### Create an LDA model using pre-processed articles. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2EZWHs98zav",
        "colab_type": "text"
      },
      "source": [
        "Using formatted articles, we create an LDA model to form topics each is represented by set of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_P-g_2O8z97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model_tfidf = gensim.models.LdaMulticore(tfidf_converted_articles, num_topics=num_topics, id2word=dictionary, passes=2, workers=4)\n",
        "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKi174p8D1Th",
        "colab_type": "text"
      },
      "source": [
        "Save the LDA model and BoW dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_imcIzQD1tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save lda model\n",
        "# lda_model_tfidf.save(os.path.join(PATH, \"lda-tfidf-model\"))\n",
        "\n",
        "# save bow dictionary\n",
        "dictionary.save(os.path.join(PATH, \"bow-dictionary\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psSe3gg0FYP3",
        "colab_type": "text"
      },
      "source": [
        "###### Download already created LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXlUywLiFZ1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "2d08a6f4-f506-4d27-b850-98d0808c2aa5"
      },
      "source": [
        "# load the pretrained lda model\n",
        "!wget -O isentia_lda_model.zip -P PATH https://www.dropbox.com/s/t3td12qefngddo0/isentia-lda-model.zip?dl=1\n",
        "!unzip isentia_lda_model.zip\n",
        "fname = datapath(os.path.join(PATH, \"lda-tfidf-model\"))\n",
        "lda_model_tfidf = models.LdaModel.load(fname, mmap='r')\n",
        "\n",
        "# load bow dictionary\n",
        "!wget -O bow-dictionary -P PATH https://www.dropbox.com/s/t3td12qefngddo0/isentia-lda-model.zip?dl=1\n",
        "fname = datapath(os.path.join(PATH, \"bow-dictionary\"))\n",
        "dictionary = gensim.corpora.Dictionary.load(fname, mmap='r')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-02 18:30:34--  https://www.dropbox.com/s/t3td12qefngddo0/isentia-lda-model.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.83.1, 2620:100:6033:1::a27d:5301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.83.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/t3td12qefngddo0/isentia-lda-model.zip [following]\n",
            "--2020-09-02 18:30:34--  https://www.dropbox.com/s/dl/t3td12qefngddo0/isentia-lda-model.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com/cd/0/get/A-m_m14t2fzz4XUhY1M0zpkKqMHCzI5e0FqDV9Yy-peBn9_LJL2uMgb1mGfddZB4eEyzbIxd6IaUXUCuYLhd0ObPuJavtnhjWutmcXV3cQIyyhiQqNw3RJ3Sj6svFr5vQwA/file?dl=1# [following]\n",
            "--2020-09-02 18:30:34--  https://uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com/cd/0/get/A-m_m14t2fzz4XUhY1M0zpkKqMHCzI5e0FqDV9Yy-peBn9_LJL2uMgb1mGfddZB4eEyzbIxd6IaUXUCuYLhd0ObPuJavtnhjWutmcXV3cQIyyhiQqNw3RJ3Sj6svFr5vQwA/file?dl=1\n",
            "Resolving uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com (uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com)... 162.125.83.15, 2620:100:6033:15::a27d:530f\n",
            "Connecting to uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com (uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com)|162.125.83.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9827956 (9.4M) [application/binary]\n",
            "Saving to: ‘isentia_lda_model.zip’\n",
            "\n",
            "100%[======================================>] 9,827,956   19.4MB/s   in 0.5s   \n",
            "\n",
            "2020-09-02 18:30:36 (19.4 MB/s) - ‘isentia_lda_model.zip’ saved [9827956/9827956]\n",
            "\n",
            "Archive:  isentia_lda_model.zip\n",
            "   creating: isentia-lda-model/\n",
            "  inflating: isentia-lda-model/lda-tfidf-model  \n",
            "  inflating: __MACOSX/isentia-lda-model/._lda-tfidf-model  \n",
            "  inflating: isentia-lda-model/lda-tfidf-model.state  \n",
            "  inflating: __MACOSX/isentia-lda-model/._lda-tfidf-model.state  \n",
            "  inflating: isentia-lda-model/lda-tfidf-model.expElogbeta.npy  \n",
            "  inflating: __MACOSX/isentia-lda-model/._lda-tfidf-model.expElogbeta.npy  \n",
            "  inflating: isentia-lda-model/lda-tfidf-model.id2word  \n",
            "  inflating: __MACOSX/isentia-lda-model/._lda-tfidf-model.id2word  \n",
            "--2020-09-02 18:30:36--  https://www.dropbox.com/s/t3td12qefngddo0/isentia-lda-model.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.83.1, 2620:100:6033:1::a27d:5301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.83.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/t3td12qefngddo0/isentia-lda-model.zip [following]\n",
            "--2020-09-02 18:30:37--  https://www.dropbox.com/s/dl/t3td12qefngddo0/isentia-lda-model.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com/cd/0/get/A-m_m14t2fzz4XUhY1M0zpkKqMHCzI5e0FqDV9Yy-peBn9_LJL2uMgb1mGfddZB4eEyzbIxd6IaUXUCuYLhd0ObPuJavtnhjWutmcXV3cQIyyhiQqNw3RJ3Sj6svFr5vQwA/file?dl=1# [following]\n",
            "--2020-09-02 18:30:37--  https://uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com/cd/0/get/A-m_m14t2fzz4XUhY1M0zpkKqMHCzI5e0FqDV9Yy-peBn9_LJL2uMgb1mGfddZB4eEyzbIxd6IaUXUCuYLhd0ObPuJavtnhjWutmcXV3cQIyyhiQqNw3RJ3Sj6svFr5vQwA/file?dl=1\n",
            "Resolving uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com (uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com)... 162.125.83.15, 2620:100:6033:15::a27d:530f\n",
            "Connecting to uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com (uc977dfc88c5bb13a402f54ee4f4.dl.dropboxusercontent.com)|162.125.83.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9827956 (9.4M) [application/binary]\n",
            "Saving to: ‘bow-dictionary’\n",
            "\n",
            "100%[======================================>] 9,827,956   19.7MB/s   in 0.5s   \n",
            "\n",
            "2020-09-02 18:30:38 (19.7 MB/s) - ‘bow-dictionary’ saved [9827956/9827956]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP59zox_Pa3m",
        "colab_type": "text"
      },
      "source": [
        "##### Assign topics to articles using LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxG6pXEjPaAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_lda_topics = []\n",
        "for article in tfidf_converted_articles:\n",
        "  topic_num, similarity_score = sorted(lda_model_tfidf[article], key=lambda item: item[1], reverse=True)[0]\n",
        "  article_lda_topics.append(topic_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sg_3CD5Qvjn",
        "colab_type": "text"
      },
      "source": [
        "##### Create a dataframe contains pre-processed articles, their initial labels (not reliable) and lda clustered topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7q6vq6RQs5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert lists to dataframe\n",
        "df = pd.DataFrame([\" \".join(article) for article in processed_articles], columns=[\"article\"])\n",
        "df[\"initial_label\"] = labels\n",
        "df[\"lda_topic\"] = article_lda_topics\n",
        "\n",
        "# remove rows with missing article values (if any)\n",
        "df.dropna(axis=0, inplace=True)\n",
        "\n",
        "# drop very short articles with less than 50 words\n",
        "short_articles = df.loc[df.article.str.split().str.len() < 50]\n",
        "df.drop(short_articles.index, axis=0, inplace=True)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY6qIYxc9Fby",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Download already pre-processed articles in CSV format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qe7eBqubyRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bf9fa785-47cf-4209-fb6c-8e8600e2d880"
      },
      "source": [
        "# download processed articles\n",
        "!wget -O articles.csv -P PATH https://www.dropbox.com/s/qbkm39innbud1c4/articles.csv?dl=1\n",
        "\n",
        "# load articles\n",
        "df = pd.read_csv(os.path.join(\".\", \"articles.csv\"))\n",
        "\n",
        "# remove rows with missing article values (if any)\n",
        "df.dropna(axis=0, inplace=True)\n",
        "\n",
        "# drop very short articles with less than 50 words\n",
        "short_articles = df.loc[df.article.str.split().str.len() < 50]\n",
        "df.drop(short_articles.index, axis=0, inplace=True)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             article  \\\n",
              "0  investment accelerate divergent efforts global...   \n",
              "1  interpol hunting fugitive queensland nickel di...   \n",
              "2  things victoria driving grunt work today host ...   \n",
              "3  thanks small mercies shorten believes economic...   \n",
              "4  heart australia future kids members guests iso...   \n",
              "\n",
              "                               initial_label  lda_topic  \n",
              "0                 Energy&Resources&Utilities          7  \n",
              "1                              Legal&Defence          3  \n",
              "2                              Entertainment          9  \n",
              "3                              Food&Beverage          2  \n",
              "4  Information,Technology&Telecommunications          7  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>initial_label</th>\n",
              "      <th>lda_topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>investment accelerate divergent efforts global...</td>\n",
              "      <td>Energy&amp;Resources&amp;Utilities</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>interpol hunting fugitive queensland nickel di...</td>\n",
              "      <td>Legal&amp;Defence</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>things victoria driving grunt work today host ...</td>\n",
              "      <td>Entertainment</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>thanks small mercies shorten believes economic...</td>\n",
              "      <td>Food&amp;Beverage</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>heart australia future kids members guests iso...</td>\n",
              "      <td>Information,Technology&amp;Telecommunications</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrHy3eHbF3S8",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare articles to train our transformer:\n",
        "\n",
        "\n",
        "1. Tokenization and Padding\n",
        "3. Split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwEAr3DXRm15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles = df.article.values\n",
        "article_labels = np.array(list(df.lda_topic.values))\n",
        "num_labels = df.lda_topic.nunique()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW9uzPwK-Z8R",
        "colab_type": "text"
      },
      "source": [
        "Tokenize articles using *BERTTokenizer*.\n",
        "\n",
        "While BERT accepts sentences with maximum 512 words length, around *19k* articles have long texts (*more than 512 words*). In order to fix this limitation, we need to split long texts into small chunks. For now,\n",
        "we pad all sentences into the first 256 to speed up training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfQGGQeDcDnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load tokenizer\n",
        "model_option = \"bert-base-uncased\" # we use base model\n",
        "tokenizer = BertTokenizer.from_pretrained(model_option)\n",
        "\n",
        "# set the maximum length for sequences, I set the length to 256 to speed up training\n",
        "MAX_LEN = 256 # maximum length of text that BERT accepts is 512, however we need to fix this limitation by splitting long texts into smaller chunks\n",
        "# MAX_LEN = df.article.str.split().str.len().mean()\n",
        "# long_articles = df.loc[df.article.str.split().str.len() > 512]\n",
        "\n",
        "# use encode plus to perform tokenization, padding and masking\n",
        "encodings = tokenizer.batch_encode_plus(articles[:50000], max_length=MAX_LEN, padding=\"max_length\", truncation=True)\n",
        "\n",
        "input_ids = encodings[\"input_ids\"]\n",
        "token_type_ids = encodings[\"token_type_ids\"]\n",
        "attention_masks = encodings[\"attention_mask\"]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZimzguVq_XNX",
        "colab_type": "text"
      },
      "source": [
        "##### Split articles into train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwtsVIZidS6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split samples into train, validation, test\n",
        "train_inputs, test_inputs, train_labels, test_labels, train_attention_masks, test_attention_masks = train_test_split(input_ids, article_labels[:50000], attention_masks, test_size=0.1, random_state=42, shuffle=True)\n",
        "train_inputs, validation_inputs, train_labels, validation_labels, train_attention_masks, validation_attention_masks = train_test_split(train_inputs, train_labels, train_attention_masks, test_size=0.1, random_state=42, shuffle=True)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzfQ_orDta-M",
        "colab_type": "text"
      },
      "source": [
        "Convert all lists to torch tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_UbsQyCte1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all to tensors\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "train_attention_masks = torch.tensor(train_attention_masks)\n",
        "# train_token_types = torch.tensor(train_token_types)\n",
        "\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "validation_attention_masks = torch.tensor(validation_attention_masks)\n",
        "# validation_token_types = torch.tensor(validation_token_types)\n",
        "\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
        "test_attention_masks = torch.tensor(test_attention_masks)\n",
        "# test_token_types = torch.tensor(test_token_types)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB0B0gnvtkXv",
        "colab_type": "text"
      },
      "source": [
        "Create Dataloader to save memory, no need to load all the data into memory at once (during training/validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjg07I0PtkFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Dataloader to save memory\n",
        "batch_size = 64\n",
        "train_data = TensorDataset(train_inputs, train_labels, train_attention_masks)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_labels, validation_attention_masks)\n",
        "validation_sampler = RandomSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_labels, test_attention_masks)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ma0Xg-etyix",
        "colab_type": "text"
      },
      "source": [
        "### Build our transformer model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7rg0nmZ0xbn",
        "colab_type": "text"
      },
      "source": [
        "Create **BertArticleClassifier** class inherit from BERTModel with a classifier as last layer on top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17h7mWJYty6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertArticleClassifier(nn.Module):\n",
        "  def __init__(self, config, num_labels, model_option):\n",
        "    super(BertArticleClassifier, self).__init__()\n",
        "    self.num_labels = num_labels\n",
        "    self.bert = BertModel.from_pretrained(model_option)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.classifier = nn.Linear(in_features=config.hidden_size, out_features=num_labels)\n",
        "    nn.init.xavier_uniform_(self.classifier.weight)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
        "    sequence_output, pooled_output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "    output = self.dropout(pooled_output)\n",
        "    logits = self.classifier(output)\n",
        "    return (logits, pooled_output)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJwyY8ROv3bd",
        "colab_type": "text"
      },
      "source": [
        "Create our Article Classifer transformer model and load it into GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN-kxVpGvjJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b205f0c9-1861-45af-e230-1499c0333a06"
      },
      "source": [
        "model_option = \"bert-base-uncased\"\n",
        "model = BertArticleClassifier(BertConfig(model_option), num_labels, model_option)\n",
        "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# load the model into gpu (if any)\n",
        "model.to(device)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertArticleClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=23, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDwzuXfyv5v8",
        "colab_type": "text"
      },
      "source": [
        "Set fine-tuning parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J0wBKgZv6Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set fine-tuning parameters\n",
        "epochs = 5\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0},\n",
        "    ]\n",
        "\n",
        "# we use adamw optimizer with learning rate 3e-5 for now\n",
        "optimizer = optim.AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
        "\n",
        "# add warmup schedule\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbPYpE6Sw9f4",
        "colab_type": "text"
      },
      "source": [
        "Train and Validate our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1f5zUoGw8iL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad495914-d1b4-49cf-9d89-1c7bc27d9e02"
      },
      "source": [
        "def train_model(model, optimizer, scheduler, num_epochs):\n",
        "  loss_values = []\n",
        "  valid_loss_values = []\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  for _ in trange(num_epochs, desc=\"Epochs==>\"):\n",
        "    # start time for each training epoch.\n",
        "    t0 = time.time()\n",
        "    \n",
        "    # reset the losses for this epoch.\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    # put the model into training mode.\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      \n",
        "      # print the progress for every 100 batches\n",
        "      if step % 100 ==0 and not step ==0:\n",
        "        elapsed_rounded = int(round((time.time() - t0)))\n",
        "        elapsed = str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "        print(\"\")\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "      \n",
        "      # load the batch into gpu (otherwise into cpu)\n",
        "      batch = [t.to(device) for t in batch]\n",
        "      \n",
        "      # unpack the batch\n",
        "      b_input_ids, b_labels, b_attention_masks = batch\n",
        "      \n",
        "      # clear any gradients accumulated from previous turn\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      logits, _ = model(b_input_ids, attention_mask = b_attention_masks)\n",
        "      # logits, = model(b_input_ids, attention_mask = b_attention_masks)\n",
        "      loss = loss_func(logits.view(-1, num_labels), b_labels.view(-1))\n",
        "      train_loss += loss.item()\n",
        "      \n",
        "      # calculate the gradients\n",
        "      loss.backward()\n",
        "      \n",
        "      # just to make sure we won't have gradient exploding issue, we clip it to 1\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      # update weights according to the gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # update learning rate\n",
        "      scheduler.step()\n",
        "    \n",
        "    avg_loss = train_loss/len(train_dataloader)\n",
        "    loss_values.append(avg_loss) # keep the average loss for this round\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_loss))\n",
        "    elapsed_rounded = int(round((time.time() - t0)))\n",
        "    elapsed = str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "    print(\"  Training epoch took: {:}\".format(elapsed))\n",
        "\n",
        "    # validate our model\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "    t0 = time.time()\n",
        "\n",
        "    # put the model in evaluation mode\n",
        "    model.eval()\n",
        "    for step, batch in enumerate(validation_dataloader):\n",
        "      batch = [t.to(device) for t in batch]\n",
        "      b_input_ids, b_labels, b_attention_masks = batch\n",
        "      with torch.no_grad():\n",
        "        logits, _ = model(b_input_ids, attention_mask = b_attention_masks)\n",
        "        # logits,  = model(b_input_ids, attention_mask = b_attention_masks)\n",
        "\n",
        "      loss = loss_func(logits.view(-1, num_labels), b_labels.view(-1))\n",
        "      valid_loss += loss.item()\n",
        "      \n",
        "      # move the logits into cpu as we don't need to keep them in gpu anymore\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      b_labels = b_labels.to(\"cpu\").numpy()\n",
        "      pred_labels.extend(np.argmax(logits, axis=1).flatten()) # keep predicted labels\n",
        "      true_labels.extend(b_labels.flatten()) # keep true labels\n",
        "\n",
        "    avg_loss = valid_loss/len(validation_dataloader)\n",
        "    valid_loss_values.append(avg_loss) # keep the average loss for this round\n",
        "    print(\"\")\n",
        "    print(\"  Average validation loss: {0:.2f}\".format(avg_loss))\n",
        "    print(\"  Validation accuracy: {0:.2f}\".format(accuracy_score(true_labels, pred_labels)))\n",
        "\n",
        "    elapsed_rounded = int(round((time.time() - t0)))\n",
        "    elapsed = str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "    print(\"  Validation epoch took: {:}\".format(elapsed))\n",
        "  return loss_values, valid_loss_values\n",
        "\n",
        "\n",
        "loss_values, valid_loss_values = train_model(model, optimizer, scheduler, epochs)\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epochs==>:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  Batch   100  of    633.    Elapsed: 0:01:35.\n",
            "\n",
            "  Batch   200  of    633.    Elapsed: 0:03:13.\n",
            "\n",
            "  Batch   300  of    633.    Elapsed: 0:05:01.\n",
            "\n",
            "  Batch   400  of    633.    Elapsed: 0:06:55.\n",
            "\n",
            "  Batch   500  of    633.    Elapsed: 0:08:51.\n",
            "\n",
            "  Batch   600  of    633.    Elapsed: 0:10:50.\n",
            "\n",
            "  Average training loss: 0.89\n",
            "  Training epoch took: 0:11:31\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epochs==>:  20%|██        | 1/5 [11:58<47:55, 718.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  Average validation loss: 0.64\n",
            "  Validation accuracy: 0.80\n",
            "  Validation epoch took: 0:00:28\n",
            "\n",
            "  Batch   100  of    633.    Elapsed: 0:02:01.\n",
            "\n",
            "  Batch   200  of    633.    Elapsed: 0:04:02.\n",
            "\n",
            "  Batch   300  of    633.    Elapsed: 0:06:03.\n",
            "\n",
            "  Batch   400  of    633.    Elapsed: 0:08:06.\n",
            "\n",
            "  Batch   500  of    633.    Elapsed: 0:10:09.\n",
            "\n",
            "  Batch   600  of    633.    Elapsed: 0:12:10.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epoch took: 0:12:51\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epochs==>:  40%|████      | 2/5 [25:17<37:08, 742.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  Average validation loss: 0.59\n",
            "  Validation accuracy: 0.80\n",
            "  Validation epoch took: 0:00:28\n",
            "\n",
            "  Batch   100  of    633.    Elapsed: 0:02:03.\n",
            "\n",
            "  Batch   200  of    633.    Elapsed: 0:04:06.\n",
            "\n",
            "  Batch   300  of    633.    Elapsed: 0:06:08.\n",
            "\n",
            "  Batch   400  of    633.    Elapsed: 0:08:11.\n",
            "\n",
            "  Batch   500  of    633.    Elapsed: 0:10:12.\n",
            "\n",
            "  Batch   600  of    633.    Elapsed: 0:12:15.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0:12:54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epochs==>:  60%|██████    | 3/5 [38:39<25:21, 760.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  Average validation loss: 0.58\n",
            "  Validation accuracy: 0.82\n",
            "  Validation epoch took: 0:00:28\n",
            "\n",
            "  Batch   100  of    633.    Elapsed: 0:02:02.\n",
            "\n",
            "  Batch   200  of    633.    Elapsed: 0:04:06.\n",
            "\n",
            "  Batch   300  of    633.    Elapsed: 0:06:09.\n",
            "\n",
            "  Batch   400  of    633.    Elapsed: 0:08:09.\n",
            "\n",
            "  Batch   500  of    633.    Elapsed: 0:10:12.\n",
            "\n",
            "  Batch   600  of    633.    Elapsed: 0:12:13.\n",
            "\n",
            "  Average training loss: 0.24\n",
            "  Training epoch took: 0:12:54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epochs==>:  80%|████████  | 4/5 [52:00<12:52, 772.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  Average validation loss: 0.60\n",
            "  Validation accuracy: 0.82\n",
            "  Validation epoch took: 0:00:27\n",
            "\n",
            "  Batch   100  of    633.    Elapsed: 0:02:02.\n",
            "\n",
            "  Batch   200  of    633.    Elapsed: 0:04:04.\n",
            "\n",
            "  Batch   300  of    633.    Elapsed: 0:06:06.\n",
            "\n",
            "  Batch   400  of    633.    Elapsed: 0:08:07.\n",
            "\n",
            "  Batch   500  of    633.    Elapsed: 0:10:09.\n",
            "\n",
            "  Batch   600  of    633.    Elapsed: 0:12:10.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epoch took: 0:12:50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epochs==>: 100%|██████████| 5/5 [1:05:18<00:00, 780.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  Average validation loss: 0.64\n",
            "  Validation accuracy: 0.82\n",
            "  Validation epoch took: 0:00:28\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeoK3egxyxar",
        "colab_type": "text"
      },
      "source": [
        "Plot Training/Validation loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmA1UWdfy8ow",
        "colab_type": "text"
      },
      "source": [
        "Test our model using the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uxBWO2Zy8RT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6f85d677-f85c-4e73-fc98-ef172738f1a2"
      },
      "source": [
        "print('{:,} Test samples...'.format(test_inputs.shape[0]))\n",
        "\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "test_pred_labels = []\n",
        "test_true_labels = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  # load the batch into gpu\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # unpack the inputs from our test dataloader\n",
        "  b_input_ids, b_labels, b_attention_masks = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      logits, _ = model(b_input_ids, attention_mask = b_attention_masks)\n",
        "\n",
        "  # move logits and intents to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  b_labels = b_labels.to(\"cpu\").numpy()  \n",
        "  \n",
        "  test_pred_labels.extend(np.argmax(logits, axis=1).flatten())\n",
        "  test_true_labels.extend(b_labels.flatten())\n",
        "\n",
        "print(\"Test accuracy: {0:.2f}\".format(accuracy_score(test_true_labels, test_pred_labels)))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5,000 Test samples...\n",
            "Test accuracy: 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx4EuBaH-So2",
        "colab_type": "text"
      },
      "source": [
        "Model achieved accuracy **83%** for now, considering that we only trained it using 50k samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us8jrMIJeY9T",
        "colab_type": "text"
      },
      "source": [
        "### Try our model with new unseen articles (inference time): BERT Classifier vs LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9RpdJqMegy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# take unseen article from test.txt file\n",
        "unseen_article = 'SEPHORA OPENS AT HIGHPOINT SHOPPING CENTRE ON 2 NOVEMBER Global beauty giant SEPHORA has today announced its next store location, continuing its extensive retail expansion in Australia, opening at Highpoint Shopping Centre on November 2nd 2017. This will be SEPHORA’s 13th Australian store and third in Victoria, (Melbourne Central and Chadstone). SEPHORA has been in high demand with Victorian beauty lovers since the opening of its first Victorian store, Melbourne Central in 2015. SEPHORA enthusiasts will be able to shop over 100 cosmetic brands in the new Highpoint store including exclusive lines from Marc Jacobs Beauty, Givenchy, Tarte, Anastasia Beverly Hills and the new Fenty Beauty by Rihanna As SEPHORA’s retail expansion continues, the product offering in-store and online continues to grow, with the recent launch of its new Wellness Category. Aimed to promote beauty from the inside out, the all-new Wellness Category features leading health and wellness brands including; KORA Organics, The Beauty Chef & WellCo, which will also play a part of the Highpoint store offering. SEPHORA Highpoint Shopping Centre, 120-200 Rosamund Road, Maribyrnong 3032'"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEspmO6ajMEC",
        "colab_type": "text"
      },
      "source": [
        "Ask from LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqXFVEPQjODW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61a462cd-664a-47a1-f4b2-c675290e2747"
      },
      "source": [
        "# load the pretrained lda model\n",
        "!wget -O isentia_lda_model.zip -P PATH https://www.dropbox.com/s/t3td12qefngddo0/isentia-lda-model.zip?dl=1\n",
        "!unzip isentia_lda_model.zip\n",
        "fname = datapath(os.path.join(PATH, \"lda-tfidf-model\"))\n",
        "lda_model_tfidf = models.LdaModel.load(fname, mmap='r')\n",
        "\n",
        "# load bow dictionary\n",
        "!wget -O bow-dictionary -P PATH https://www.dropbox.com/s/9a5t87a6mgw7zno/bow-dictionary?dl=1\n",
        "fname = datapath(os.path.join(PATH, \"bow-dictionary\"))\n",
        "dictionary = gensim.corpora.Dictionary.load(fname, mmap='r')\n",
        "\n",
        "def preprocess(article):\n",
        "    result = []\n",
        "    article = re.sub('[,\\.!?]', '', article)\n",
        "    for word in gensim.utils.simple_preprocess(article):\n",
        "        if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 3: # remove stop words and words with less than 3 characters\n",
        "            result.append(word)\n",
        "    return result\n",
        "\n",
        "article_bow_vector = dictionary.doc2bow(preprocess(unseen_article))\n",
        "for index, score in sorted(lda_model_tfidf[article_bow_vector], key=lambda item: item[1], reverse=True)[0:1]:\n",
        "    print(\"Topic {}\".format(index))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q9x_bTLk5Av",
        "colab_type": "text"
      },
      "source": [
        "Ask from our BERT Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6ugT_utk6QX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "777e062a-8436-4daf-9406-e75c6442d17f"
      },
      "source": [
        "# convert articles to sequences\n",
        "MAX_LEN = 512\n",
        "\n",
        "# load bert tokenizer, in case we skipped the \"building bert model\" section\n",
        "model_option = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_option)\n",
        "\n",
        "# tokenize, pad and mask the article\n",
        "encodings = tokenizer.batch_encode_plus([unseen_article], max_length = MAX_LEN, padding=True, pad_to_max_length=True, truncation=True)\n",
        "input_ids = encodings[\"input_ids\"]\n",
        "attention_masks = encodings[\"attention_mask\"]\n",
        "\n",
        "# convert to tensors\n",
        "input_ids = torch.tensor(input_ids)\n",
        "attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# move ids and masks into gpu\n",
        "input_ids = input_ids.to(device)\n",
        "attention_masks = attention_masks.to(device)\n",
        "\n",
        "# since it's evaluation, we don't need to calculate gradients\n",
        "with torch.no_grad():\n",
        "    logits, _ = model(input_ids, attention_mask = attention_masks)\n",
        "\n",
        "# move logit back to CPU\n",
        "logits = logits.detach().cpu().numpy()\n",
        "\n",
        "print(\"Topic {}\".format(np.argmax(logits, axis=1).flatten()[0]))\n"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ3WzPIyaazR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
